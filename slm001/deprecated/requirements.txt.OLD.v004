# Core PyTorch and Transformers - essential dependencies
torch>=1.9.0
transformers>=4.20.0
tokenizers>=0.12.0

# Core dependencies
numpy>=1.21.0
requests>=2.25.0
tqdm>=4.62.0
packaging>=21.0
filelock>=3.7.0
huggingface-hub>=0.8.0
pyyaml>=5.4.0
regex>=2021.8.0
sacremoses>=0.0.45
safetensors>=0.3.0

# System monitoring (optional but recommended for Jetson Nano)
psutil>=5.8.0

# Additional optimizations for ARM64/Jetson
scipy>=1.7.0

# Quantization support (optional - will be handled gracefully if not available)
# accelerate is more stable across different architectures
accelerate>=0.16.0

# Note: bitsandbytes is NOT included in requirements.txt because:
# 1. It may not be compatible with all CPU architectures
# 2. It requires specific system configurations
# 3. The application has proper fallback to PyTorch native quantization
# 
# If you want to try bitsandbytes on a compatible system, install it separately:
# pip install bitsandbytes>=0.37.0
#
# The application will automatically detect and use it if available,
# otherwise it will fall back to PyTorch native quantization.