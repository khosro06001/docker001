# Core PyTorch and Transformers
torch>=1.9.0
transformers>=4.20.0
tokenizers>=0.12.0

# Quantization support (optional - will fallback gracefully if not available)
# Note: bitsandbytes may not work on all architectures
# The application will use PyTorch native quantization as fallback
accelerate>=0.16.0

# Core dependencies
numpy>=1.21.0
requests>=2.25.0
tqdm>=4.62.0
packaging>=21.0
filelock>=3.7.0
huggingface-hub>=0.8.0
pyyaml>=5.4.0
regex>=2021.8.0
sacremoses>=0.0.45
safetensors>=0.3.0

# System monitoring (optional but recommended for Jetson Nano)
psutil>=5.8.0

# Additional optimizations for ARM64/Jetson
scipy>=1.7.0

# Try to install bitsandbytes but don't fail if it's not compatible
# Uncomment the next line if you want to try bitsandbytes installation
# bitsandbytes>=0.37.0; sys_platform != "darwin" or platform_machine != "arm64"