#!/bin/bash

# Multi-architecture Docker build and deployment script for slm001 quantized chatbot
# This script builds for both AMD64 and ARM64 architectures with quantization support

set -e  # Exit on any error

# Configuration
DOCKER_USERNAME="khosro123"  # Replace with your Docker Hub username
IMAGE_NAME="slm001-quantized-chatbot"
TAG="latest"
FULL_IMAGE_NAME="${DOCKER_USERNAME}/${IMAGE_NAME}:${TAG}"

# Additional tags for different versions
QUANTIZED_TAG="quantized"
JETSON_TAG="jetson-nano"
FULL_QUANTIZED_NAME="${DOCKER_USERNAME}/${IMAGE_NAME}:${QUANTIZED_TAG}"
FULL_JETSON_NAME="${DOCKER_USERNAME}/${IMAGE_NAME}:${JETSON_TAG}"

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
NC='\033[0m' # No Color

print_status() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

print_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

print_feature() {
    echo -e "${PURPLE}[FEATURE]${NC} $1"
}

# Function to display quantization features
show_quantization_features() {
    print_feature "=== QUANTIZED CHATBOT FEATURES ==="
    print_feature "üî• Intelligent quantization fallback system"
    print_feature "‚ö° Optimized for Jetson Nano (4GB RAM)"
    print_feature "üöÄ Multiple quantized model options:"
    print_feature "   ‚Ä¢ DialoGPT-small (~117MB)"
    print_feature "   ‚Ä¢ DistilGPT2 (~320MB)"
    print_feature "   ‚Ä¢ GPT2 (~500MB)"
    print_feature "   ‚Ä¢ DialoGPT-medium (~350MB)"
    print_feature "   ‚Ä¢ TinyLlama-1.1B (~1.1GB)"
    print_feature "üíæ Memory-efficient conversation handling"
    print_feature "üéØ Real-time model switching capability"
    print_feature "üîß Graceful quantization fallback (bitsandbytes -> PyTorch native)"
    echo ""
}

# Function to check if docker is installed and running
check_docker() {
    if ! command -v docker &> /dev/null; then
        print_error "Docker is not installed. Please install Docker first."
        exit 1
    fi
    
    if ! docker info &> /dev/null; then
        print_error "Docker is not running. Please start Docker daemon."
        exit 1
    fi
    
    print_success "Docker is installed and running"
}

# Function to check if buildx is available
check_buildx() {
    if ! docker buildx version &> /dev/null; then
        print_error "Docker buildx is not available. Please update Docker to a newer version."
        exit 1
    fi
    
    print_success "Docker buildx is available"
}

# Function to check required files
check_files() {
    print_status "Checking required files..."
    
    required_files=("slm001.py" "requirements.txt")
    dockerfile_found=false
    
    # Check for Dockerfile or Dockerfile.txt
    if [ -f "Dockerfile" ]; then
        dockerfile_found=true
    elif [ -f "Dockerfile.txt" ]; then
        dockerfile_found=true
        print_status "Found Dockerfile.txt, will use it for building"
    fi
    
    missing_files=()
    
    for file in "${required_files[@]}"; do
        if [ ! -f "$file" ]; then
            missing_files+=("$file")
        fi
    done
    
    if [ "$dockerfile_found" = false ]; then
        missing_files+=("Dockerfile or Dockerfile.txt")
    fi
    
    if [ ${#missing_files[@]} -gt 0 ]; then
        print_error "Missing required files: ${missing_files[*]}"
        print_error "Make sure you have all required files in the current directory"
        exit 1
    fi
    
    # Use Dockerfile if it exists, otherwise create from Dockerfile.txt
    if [ -f "Dockerfile.txt" ] && [ ! -f "Dockerfile" ]; then
        print_status "Using Dockerfile.txt for build (no need to rename)"
        DOCKERFILE_NAME="Dockerfile.txt"
    else
        DOCKERFILE_NAME="Dockerfile"
    fi
    
    print_success "All required files are present"
}

# Function to create and use buildx builder
setup_buildx() {
    print_status "Setting up Docker buildx for multi-architecture builds..."
    
    # Create a new builder instance
    docker buildx create --name multiarch-builder --use --bootstrap 2>/dev/null || true
    
    # Use the builder
    docker buildx use multiarch-builder
    
    # Inspect the builder
    docker buildx inspect --bootstrap
    
    print_success "Multi-architecture builder setup complete"
}

# Function to login to Docker Hub
docker_login() {
    print_status "Logging in to Docker Hub..."
    
    if [ -z "$DOCKER_USERNAME" ] || [ "$DOCKER_USERNAME" = "your_dockerhub_username" ]; then
        print_error "Please set your Docker Hub username in the script"
        exit 1
    fi
    
    echo "Please enter your Docker Hub password:"
    docker login -u "$DOCKER_USERNAME"
    
    print_success "Successfully logged in to Docker Hub"
}

# Function to build multi-architecture quantized image
build_image() {
    print_status "Building multi-architecture quantized Docker image..."
    print_status "Base Image: $FULL_IMAGE_NAME"
    print_status "Quantized Tag: $FULL_QUANTIZED_NAME"
    print_status "Jetson Tag: $FULL_JETSON_NAME"
    print_status "Architectures: linux/amd64, linux/arm64"
    print_status "Using Dockerfile: $DOCKERFILE_NAME"
    print_feature "Quantization: Intelligent fallback system included"
    
    # Build and push for multiple architectures with multiple tags
    docker buildx build \
        --platform linux/amd64,linux/arm64 \
        --tag "$FULL_IMAGE_NAME" \
        --tag "$FULL_QUANTIZED_NAME" \
        --tag "$FULL_JETSON_NAME" \
        --file "$DOCKERFILE_NAME" \
        --push \
        .
    
    print_success "Multi-architecture quantized image built and pushed successfully"
}

# Function to verify the image
verify_image() {
    print_status "Verifying multi-architecture quantized image..."
    
    # Check if the images exist and show manifest
    for image in "$FULL_IMAGE_NAME" "$FULL_QUANTIZED_NAME" "$FULL_JETSON_NAME"; do
        print_status "Checking manifest for: $image"
        docker buildx imagetools inspect "$image"
        echo ""
    done
    
    print_success "Image verification complete"
}

# Function to test image locally
test_image_local() {
    print_status "Testing image locally (if supported architecture)..."
    
    # Get current architecture
    ARCH=$(uname -m)
    if [[ "$ARCH" == "x86_64" ]] || [[ "$ARCH" == "aarch64" ]] || [[ "$ARCH" == "arm64" ]]; then
        print_status "Testing on $ARCH architecture..."
        
        # Run a quick test
        docker run --rm "$FULL_IMAGE_NAME" python -c "
import sys
print('Python version:', sys.version)
try:
    import torch
    print('PyTorch version:', torch.__version__)
    print('CUDA available:', torch.cuda.is_available())
except ImportError as e:
    print('PyTorch import error:', e)

try:
    import transformers
    print('Transformers version:', transformers.__version__)
except ImportError as e:
    print('Transformers import error:', e)

try:
    import bitsandbytes
    print('BitsAndBytes available: Yes')
except ImportError:
    print('BitsAndBytes available: No (will use PyTorch native quantization)')

print('‚úÖ Basic dependencies test passed!')
print('üîß Quantization fallback system ready!')
"
        print_success "Local test completed successfully"
    else
        print_warning "Skipping local test - unsupported architecture: $ARCH"
    fi
}

# Function to generate comprehensive run instructions
generate_instructions() {
    print_status "Generating comprehensive run instructions..."
    
    cat << EOF > run_instructions.md
# SLM001 Quantized Chatbot Deployment Instructions

## üöÄ Quick Start

### On Jetson Nano (Recommended):
\`\`\`bash
# Pull the optimized image
docker pull ${FULL_JETSON_NAME}

# Run the quantized chatbot
docker run -it --rm ${FULL_JETSON_NAME}
\`\`\`

### On any ARM64/AMD64 device:
\`\`\`bash
# Pull the latest image
docker pull ${FULL_IMAGE_NAME}

# Run with interactive mode
docker run -it --rm ${FULL_IMAGE_NAME}
\`\`\`

## üî• Quantized Model Features

- **Intelligent Quantization Fallback**: Automatically uses best available quantization method
- **Multi-tier Quantization**: bitsandbytes ‚Üí PyTorch native ‚Üí no quantization
- **Memory Optimized**: Perfect for 4GB RAM devices like Jetson Nano
- **Real-time Model Switching**: Change models during conversation
- **Multiple Model Options**:
  - DialoGPT-small (~117MB native)
  - DistilGPT2 (~320MB native)
  - GPT2 (~500MB native)
  - DialoGPT-medium (~350MB native)
  - TinyLlama-1.1B (~1.1GB native)

## üéØ Available Commands

Once the chatbot is running:
- \`models\` - Show available quantized models
- \`switch <model_id>\` - Switch to different model
- \`stats\` - Show model and memory statistics
- \`memory\` - Show current memory usage
- \`clear\` - Clear conversation history
- \`help\` - Show help message
- \`quit\` - Exit the chatbot

## üíæ Persistent Storage Options

### With model cache persistence:
\`\`\`bash
docker run -it --rm -v slm001-cache:/app/cache ${FULL_IMAGE_NAME}
\`\`\`

### With full data persistence:
\`\`\`bash
docker run -it --rm -v slm001-data:/app/data -v slm001-cache:/app/cache ${FULL_IMAGE_NAME}
\`\`\`

## üñ•Ô∏è Advanced Usage

### Run in detached mode:
\`\`\`bash
docker run -d --name slm001-quantized ${FULL_IMAGE_NAME}
\`\`\`

### Attach to running container:
\`\`\`bash
docker exec -it slm001-quantized /bin/bash
\`\`\`

### View logs:
\`\`\`bash
docker logs slm001-quantized
\`\`\`

### Stop container:
\`\`\`bash
docker stop slm001-quantized
\`\`\`

## üîß Memory and Performance Tuning

### For Jetson Nano (4GB RAM):
\`\`\`bash
# Use memory-optimized settings
docker run -it --rm --memory=3g --memory-swap=4g ${FULL_JETSON_NAME}
\`\`\`

### For devices with more RAM:
\`\`\`bash
# Allow more memory for better performance
docker run -it --rm --memory=8g ${FULL_IMAGE_NAME}
\`\`\`

## üêõ Troubleshooting

### Check Docker version:
\`\`\`bash
docker --version
\`\`\`

### Check available architectures:
\`\`\`bash
docker buildx ls
\`\`\`

### Force re-pull image:
\`\`\`bash
docker rmi ${FULL_IMAGE_NAME}
docker pull ${FULL_IMAGE_NAME}
\`\`\`

### Check container resource usage:
\`\`\`bash
docker stats slm001-quantized
\`\`\`

### Debug mode (verbose logging):
\`\`\`bash
docker run -it --rm -e PYTHONDONTWRITEBYTECODE=0 ${FULL_IMAGE_NAME}
\`\`\`

## üìä Performance Expectations

| Device | Model | Load Time | Memory Usage | Response Time |
|--------|-------|-----------|--------------|---------------|
| Jetson Nano | DialoGPT-small | ~5-10s | ~300-500MB | ~1-3s |
| Jetson Nano | DistilGPT2 | ~10-15s | ~500-800MB | ~2-4s |
| RPi 4 (8GB) | GPT2 | ~15-20s | ~800MB-1.2GB | ~3-5s |
| x86_64 CPU | TinyLlama | ~10-15s | ~1-2GB | ~1-3s |

## üîÑ Model Switching Example

\`\`\`
üí¨ You: models
ü§ñ Bot: [Shows available quantized models]

üí¨ You: switch distilgpt2
ü§ñ Bot: Switching to DistilGPT2...
‚úÖ Successfully switched to DistilGPT2!

üí¨ You: stats
ü§ñ Bot: [Shows current model statistics]
\`\`\`

## üåê Network Requirements

- **Initial Setup**: Internet required for downloading models
- **Runtime**: No internet required after models are cached
- **Model Cache**: Stored in \`/app/cache\` (can be persisted)

## ‚ö†Ô∏è Known Limitations

- First model load requires internet connection
- Quantized models may have slightly reduced quality
- Memory usage varies by model and conversation length
- Some models may not work on all architectures

## üìã Quantization Fallback Chain

1. **bitsandbytes quantization** (if available and supported)
2. **PyTorch native quantization** (fallback for unsupported hardware)
3. **No quantization** (final fallback, uses more memory)

The system automatically detects the best available method.

## üÜò Support

If you encounter issues:
1. Check the troubleshooting section above
2. Ensure your device meets minimum requirements (2GB RAM)
3. Try different models if one doesn't work
4. Check Docker logs for detailed error messages
5. The quantization fallback system should handle most compatibility issues

---

**Image Information:**
- Repository: ${DOCKER_USERNAME}/${IMAGE_NAME}
- Tags: latest, quantized, jetson-nano
- Architectures: linux/amd64, linux/arm64
- Base: python:3.9-slim
- Quantization: Intelligent fallback system (bitsandbytes ‚Üí PyTorch native)

EOF

    print_success "Comprehensive instructions saved to run_instructions.md"
}

# Function to create a simple test script
create_test_script() {
    print_status "Creating test script..."
    
    cat << 'EOF' > test_quantized_chatbot.sh
#!/bin/bash

# Quick test script for quantized chatbot
IMAGE_NAME="khosro123/slm001-quantized-chatbot:latest"

echo "üß™ Testing Quantized Chatbot with Fallback System..."

# Test 1: Basic import test
echo "Test 1: Basic dependency check..."
docker run --rm "$IMAGE_NAME" python test_env.py

# Test 2: Quantization availability test
echo -e "\nTest 2: Quantization fallback system check..."
docker run --rm "$IMAGE_NAME" python -c "
try:
    import bitsandbytes
    print('‚úÖ BitsAndBytes available for quantization')
except ImportError:
    print('‚ö†Ô∏è  BitsAndBytes not available, using PyTorch native quantization')

try:
    from transformers import BitsAndBytesConfig
    print('‚úÖ BitsAndBytesConfig available')
except ImportError:
    print('‚ö†Ô∏è  BitsAndBytesConfig not available, using fallback')

# Test PyTorch quantization
try:
    import torch
    print('‚úÖ PyTorch available for native quantization')
    print('‚úÖ Quantization fallback chain ready')
except ImportError:
    print('‚ùå PyTorch not available')
"

# Test 3: Model configuration test
echo -e "\nTest 3: Model configuration check..."
docker run --rm "$IMAGE_NAME" python -c "
import sys
sys.path.append('/app')
try:
    from slm001 import MODEL_CONFIGS, QuantizedLocalChatbot
    print('‚úÖ Model configurations loaded successfully')
    print(f'Available models: {len(MODEL_CONFIGS)}')
    for model_key, config in MODEL_CONFIGS.items():
        print(f'  - {config[\"name\"]}: {config[\"size\"]}')
    
    # Test chatbot initialization
    chatbot = QuantizedLocalChatbot()
    print('‚úÖ Chatbot initialization successful')
    print(f'Device detected: {chatbot.device}')
    
except Exception as e:
    print(f'‚ùå Error loading model configs: {e}')
"

# Test 4: Quick interactive test
echo -e "\nTest 4: Quick interactive test (5 seconds)..."
timeout 5s docker run --rm -i "$IMAGE_NAME" python -c "
import sys
sys.path.append('/app')
from slm001 import QuantizedLocalChatbot
chatbot = QuantizedLocalChatbot()
print('‚úÖ Ready for quick test')
print('Note: Full model loading takes longer in actual use')
" 2>/dev/null || echo "‚ö†Ô∏è  Timeout reached (expected for full model loading)"

echo -e "\nüéâ Test completed!"
echo "üí° For full testing, run the chatbot interactively:"
echo "   docker run -it --rm $IMAGE_NAME"
EOF

    chmod +x test_quantized_chatbot.sh
    print_success "Test script created: test_quantized_chatbot.sh"
}

# Main execution
main() {
    print_status "Starting multi-architecture build process for quantized slm001 chatbot..."
    show_quantization_features
    
    # Check prerequisites
    check_docker
    check_buildx
    check_files
    
    # Setup buildx
    setup_buildx
    
    # Login to Docker Hub
    docker_login
    
    # Build the image
    build_image
    
    # Verify the image
    verify_image
    
    # Test locally if possible
    test_image_local
    
    # Generate instructions and test script
    generate_instructions
    create_test_script
    
    print_success "Build and deployment process completed successfully!"
    print_success "üî• Quantized chatbot images are now available:"
    print_success "   ‚Ä¢ Latest: ${FULL_IMAGE_NAME}"
    print_success "   ‚Ä¢ Quantized: ${FULL_QUANTIZED_NAME}"
    print_success "   ‚Ä¢ Jetson Nano: ${FULL_JETSON_NAME}"
    print_feature ""
    print_feature "üöÄ Quick start on Jetson Nano:"
    print_feature "   docker pull ${FULL_JETSON_NAME}"
    print_feature "   docker run -it --rm ${FULL_JETSON_NAME}"
    print_feature ""
    print_feature "üìñ Full instructions: run_instructions.md"
    print_feature "üß™ Test script: ./test_quantized_chatbot.sh"
}

# Run main function
main