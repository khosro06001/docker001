ollama run gemma2:2b-instruct-q4_0

Looking at your files, I can see you have experience with Docker and multi-architecture deployments. Before I create the scripts for your Gemma2 chatbot, let me clarify a few things to ensure I build exactly what you need:

**Architecture & Deployment:**
- ✅ Debian 12 PC AMD64 for development
- ✅ Docker Hub for distribution  
- ✅ Jetson Nano 4GB ARM64 for deployment
- ✅ Using `gemma2:2b-instruct-q4_0` model

**Chatbot Interface Questions:**
1. **Interface type**: Do you want a:
   - Command-line interactive chatbot (like a terminal chat)?
   - Simple web interface (HTML/Flask)?
   - REST API that you can call?

2. **Conversation handling**: Should it:
   - Remember conversation history within a session?
   - Start fresh with each interaction?
   - Support multiple concurrent users?

3. **Docker networking**: Based on your existing scripts, you use `--network host`. Should I:
   - Keep the same approach (host networking)?
   - Use specific port mapping?

4. **Resource constraints**: For Jetson Nano 4GB:
   - Any specific memory limits?
   - Should I include GPU acceleration or CPU-only?

5. **Input/Output**: Do you want:
   - Interactive chat (type and get responses)?
   - Batch processing capability?
   - File input/output options?

Looking at your existing pattern, I'm thinking:
- Python script with command-line interface initially
- Multi-stage Dockerfile (AMD64/ARM64)
- Similar structure to your VLM project
- Ollama integration following your existing pattern

Does this sound right? Any specific preferences for the interface or functionality?


===

1. Command-line interactive chatbot please.
2. Yes, remember.
3. yes, keep '--network host' and do whatever portmapping is most likely to succeed and not hang
4. Definitely include GPU acceleration. THIS IS CRUCIAL. (Jetson Nano ARM64 has a GPU. My Debian 12 AMD64 DOES NOT HAVE A GPU.)
5. For now, interactive chat please.

AND YES TO:
- Python script with command-line interface initially
- Multi-stage Dockerfile (AMD64/ARM64)
- Similar structure to your VLM project
- Ollama integration following your existing pattern
